{
    "nbformat_minor": 1, 
    "metadata": {
        "kernelspec": {
            "name": "python2-spark20", 
            "display_name": "Python 2 with Spark 2.0", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "version": "2.7.11", 
            "name": "python", 
            "pygments_lexer": "ipython2", 
            "nbconvert_exporter": "python", 
            "file_extension": ".py", 
            "codemirror_mode": {
                "name": "ipython", 
                "version": 2
            }
        }
    }, 
    "cells": [
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "![](https://github.com/krondor/data-science-pot/blob/master/dsx_logo.png?raw=true)\n\n# Introduction to Apache Spark: Querying data\nThis notebook guides you through querying data with Apache Spark, including how to create and use DataFrames, run SQL queries, apply functions to the results of SQL queries, join data from different data sources, and visualize data in graphs.\n\nThis notebook uses pySpark, the Python API for Spark. Some knowledge of Python is recommended. This notebook runs on Python 2 with Spark 1.6 and 2.0.\n\nIf you are new to Apache Spark, see the first module in this series: __Introduction to Apache Spark: Basic Concepts__. (need link when published)"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "## Table of contents\n\n1. [Prepare the environment and the data](#getstarted)<br>\n     1.1 [Enable SQL processing](#sqlprocessing)<br>\n     1.2 [Download the data](#download)<br>\n     1.3 [Create a DataFrame](#createdf)<br>\n     1.4 [Create a table](#createtab)<br>\n2. [Run SQL queries](#runsql)<br>\n    2.1 [Display query results with a pandas DataFrame](#pandas)<br>\n    2.2 [Run a group by query](#groupby)<br>\n    2.3 [Run a subselect query](#subselect)<br>\n    2.4 [Return nested JSON field values](#nested)<br>\n3. [Convert RDDs to DataFrames](#convertrdd)<br>\n    3.1 [Create a simple RDD](#simplerdd)<br>\n    3.2 [Apply a schema](#apply)<br>\n    3.3 [Create rows with named columns](#namedcol)<br>\n    3.4 [Join tables](#join)<br>\n4. [Create SQL functions](#sqlfuncs)<br>\n5. [Convert a pandas DataFrame to a Spark DataFrame](#sparkdf)<br>\n    5.1 [Get a new data set](#ufo)<br>\n    5.2 [Create a pandas DataFrame](#ufopandas)<br>\n    5.3 [Convert to a Spark DataFrame](#sparkufo)<br>\n    5.4 [Run an SQL statement](#runufo)<br>\n6. [Visualize data](#viz)<br>\n    6.1 [Create a chart](#vizchart)<br>\n    6.2 [Aggregate the data](#vizagg)<br>\n    6.3 [Create a better chart](#vizchart2)<br>\n7. [Summary and next steps](#nextsteps)"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "<a id=\"getstarted\"></a>\n## 1. Prepare the environment and the data\nBefore you can run SQL queries on data in an Apache Spark environment, you need to enable SQL processing and then move the data to the structured format of a DataFrame."
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "<a id=\"sqlprocessing\"></a>\n### 1.1 Enable SQL processing\nThe way you enable SQL processing with Spark 1.6 is to create an SQLContext. With Spark 2.0, the preferred method is the new SparkSession object, but the SQLContext object is still supported. \n\nUse the predefined Spark Context, `sc`, which contains the connection information for Spark to create an SQLContext:"
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "metadata": {}, 
            "cell_type": "code", 
            "source": "from pyspark.sql import SQLContext\nsqlContext = SQLContext(sc)"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "<a id=\"download\"></a>\n### 1.2 Download the data file\n\nYou'll download a JSON file with data about world banks from GitHub.\n\nRemove any files with the same name as the file that you're going to download and then download the file from a URL:"
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "metadata": {}, 
            "cell_type": "code", 
            "source": "!rm world_bank.json.gz -f\n!wget https://raw.githubusercontent.com/bradenrc/sparksql_pot/master/world_bank.json.gz"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "<a id=\"createdf\"></a>\n### 1.3 Create a DataFrame \n\nInstead of creating an RDD to read the file, you'll create a Spark DataFrame. Unlike an RDD, a DataFrame creates a schema around the data, which supplies the necessary structure for SQL queries. A self-describing format like JSON is ideal for DataFrames, but many other file types are supported, including text (CSV) and Parquet.\n\nCreate a DataFrame:"
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "metadata": {}, 
            "cell_type": "code", 
            "source": "example1_df = sqlContext.read.json(\"./world_bank.json.gz\")"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "Print the schema to see how Spark SQL inferred the shape of the data:"
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "metadata": {}, 
            "cell_type": "code", 
            "source": "print example1_df.printSchema()"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "Now look at the first two rows of data.\n\nYou can run the simple command `print example1_df.take(2)`, however, for readability, run the following command to include a row of asterisks in between the data rows:"
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "metadata": {}, 
            "cell_type": "code", 
            "source": "for row in example1_df.take(2):\n    print row\n    print \"*\" * 20"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "<a id=\"createtab\"></a>\n### 1.4 Create a table \n\nSQL statements must be run against a table. Create a table that's a pointer to the DataFrame:"
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "metadata": {}, 
            "cell_type": "code", 
            "source": "example1_df.registerTempTable(\"world_bank\")"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "<a id=\"runsql\"></a>\n## 2. Run SQL queries\n\nYou must define a new DataFrame for the results of the SQL query and put the SQL statement inside the `sqlContext.sql()` method.\n\nRun the following cell to select all columns from the table and print information about the resulting DataFrame and schema of the data:"
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "metadata": {}, 
            "cell_type": "code", 
            "source": "temp_df =  sqlContext.sql(\"select * from world_bank\")\n\nprint type(temp_df)\nprint \"*\" * 20\nprint temp_df"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "The first `print` command shows that the DataFrame is a Spark DataFrame. The last `print` command shows the column names and data types of the DataFrame."
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "<a id=\"pandas\"></a>\n### 2.1 Display query results with a pandas DataFrame\nThe `print` command doesn't show the data in a useful format. Instead of creating a Spark DataFrame, use the pandas open-source data analytics library to create a pandas DataFrame that shows the data in a table. \n\nImport the pandas library and use the `.toPandas()` method to show the query results:"
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "metadata": {}, 
            "cell_type": "code", 
            "source": "import pandas as pd\nsqlContext.sql(\"select id, borrower from world_bank limit 2\").toPandas()"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "<a id=\"groupby\"></a>\n### 2.2 Run a group by query\n\nYou can make your SQL queries easier to read by using the `query` keyword and surrounding the SQL query with `\"\"\"` on separate lines. \n\nCalculate a count of projects by region:"
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "metadata": {}, 
            "cell_type": "code", 
            "source": "query = \"\"\"\nselect\n    regionname ,\n    count(*) as project_count\nfrom world_bank\ngroup by regionname \norder by count(*) desc\n\"\"\"\n\nsqlContext.sql(query).toPandas()"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "<a id=\"subselect\"></a>\n### 2.3 Run a subselect query\nYou can run subselect queries.\n\nCalculate a count of projects by region again, but this time using a subselect:"
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "metadata": {}, 
            "cell_type": "code", 
            "source": "query = \"\"\"\n\nselect * from\n    (select\n        regionname ,\n        count(*) as project_count\n    from world_bank\n    group by regionname \n    order by count(*) desc) table_alias\nlimit 2\n\"\"\"\n\nsqlContext.sql(query).toPandas()"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "<a id=\"nested\"></a>\n### 2.4 Return nested JSON field values\nWith JSON data, you can select the values of nested fields with dot notation.\n\nPrint the schema so that you can see that `sector.Name` is a nested field and then select its first two values:"
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "metadata": {}, 
            "cell_type": "code", 
            "source": "example1_df.printSchema()\n\nsql = \"select sector.Name from world_bank limit 2\"\nsqlContext.sql(sql).show()"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "<a id=\"convertrdd\"></a>\n## 3. Convert RDDs to DataFrames\nIf you want to run SQL queries on an existing RDD, you must convert the RDD to a DataFrame. The main difference between and RDDs and DataFrames is whether the columns are named.\n\nYou'll create an RDD and then convert it to a DataFrame in two different ways:\n - [Apply a schema](#apply)\n - [Create rows with named columns](#namedcol)"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "<a id=\"simplerdd\"></a>\n### 3.1 Create a simple RDD\nYou'll create a simple RDD with an ID column and two columns of random numbers.\n\nFirst create a Python list of lists:"
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "metadata": {}, 
            "cell_type": "code", 
            "source": "import random\n\ndata_e2 = []\nfor x in range(1,6):\n    random_int = int(random.random() * 10)\n    data_e2.append([x, random_int, random_int^2])"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "Now create the RDD:"
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "metadata": {}, 
            "cell_type": "code", 
            "source": "rdd_example2 = sc.parallelize(data_e2)\nprint rdd_example2.collect()"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "<a id=\"apply\"></a>\n### 3.2 Apply a schema\nYou'll use the `StructField` method to create a schema object that's based on a string, apply the schema to the RDD to create a DataFrame, and then create a table to run SQL queries on.\n\nDefine your schema columns as a string:"
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "metadata": {}, 
            "cell_type": "code", 
            "source": "from pyspark.sql.types import *\n\nschemaString = \"ID VAL1 VAL2\""
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "Assign header information with the `StructField` method and create the schema with the `StructType` method:"
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "metadata": {}, 
            "cell_type": "code", 
            "source": "fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()]\nschema = StructType(fields)"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "Apply the schema to the RDD with the `createDataFrame` method:"
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "metadata": {}, 
            "cell_type": "code", 
            "source": "schemaExample = sqlContext.createDataFrame(rdd_example2, schema)"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "Register the DataFrame as a table:"
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "metadata": {}, 
            "cell_type": "code", 
            "source": "schemaExample.registerTempTable(\"example2\")"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "View the data:"
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "metadata": {}, 
            "cell_type": "code", 
            "source": "print schemaExample.collect()"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "You can reference the columns names in DataFrames:"
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "metadata": {}, 
            "cell_type": "code", 
            "source": "for row in schemaExample.take(2):\n    print row.ID, row.VAL1, row.VAL2"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "Run a simple SQL query:"
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "metadata": {}, 
            "cell_type": "code", 
            "source": "sqlContext.sql(\"select * from example2\").toPandas()"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "<a id=\"namedcol\"></a>\n### 3.3 Create rows with named columns\nYou'll create an RDD with named columns and then convert it to a DataFrame and a table.\n\nCreate a new RDD and specify the names of the columns with the `map` method:"
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "metadata": {}, 
            "cell_type": "code", 
            "source": "from pyspark.sql import Row\n\nrdd_example3 = rdd_example2.map(lambda x: Row(id=x[0], val1=x[1], val2=x[2]))\n\nprint rdd_example3.collect()                                                            "
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "Convert `rdd_example3` to a DataFrame and register an associated table:"
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "metadata": {}, 
            "cell_type": "code", 
            "source": "df_example3 = rdd_example3.toDF()\ndf_example3.registerTempTable(\"example3\")"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "Run a simple SQL query:"
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "metadata": {}, 
            "cell_type": "code", 
            "source": "sqlContext.sql(\"select * from example3\").toPandas()"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "<a id=\"join\"></a>\n### 3.4 Join tables\nYou can join tables.\n\nJoin tables `example2` and `example3` on the ID column:"
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "metadata": {}, 
            "cell_type": "code", 
            "source": "query = \"\"\"\nselect\n    *\nfrom\n    example2 e2\ninner join example3 e3 on\n    e2.ID = e3.id\n\"\"\"\n\nprint sqlContext.sql(query).toPandas()"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "Alternatively, you can join DataFrames with a Python command instead of an SQL query:"
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "metadata": {}, 
            "cell_type": "code", 
            "source": "df_example4 = df_example3.join(schemaExample, schemaExample[\"ID\"] == df_example3[\"id\"] )\n\nfor row in df_example4.take(5):\n    print row"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "<a id=\"sqlfuncs\"></a>\n## 4. Create SQL functions \nYou can create functions that run in SQL queries. \n\nFirst, create a Python function and test it:"
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "metadata": {}, 
            "cell_type": "code", 
            "source": "def simple_function(v):\n    return int(v * 10)\n\n#test the function\nprint simple_function(3)"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "Next, register the function as an SQL function with the `registerFunction` method:"
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "metadata": {}, 
            "cell_type": "code", 
            "source": "sqlContext.registerFunction(\"simple_function\", simple_function)"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "Now run the function in an SQL Statement:"
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "metadata": {}, 
            "cell_type": "code", 
            "source": "query = \"\"\"\nselect\n    ID,\n    VAL1,\n    VAL2,\n    simple_function(VAL1) as s_VAL1,\n    simple_function(VAL2) as s_VAL2\nfrom\n example2\n\"\"\"\nsqlContext.sql(query).toPandas()"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "The values in the VAL1 and VAL2 columns look like strings (10 characters instead of a number multiplied by 10). That's because string is the default data type for columns in Spark DataFrames.\n\nCast the values in the VAL1 and VAL2 columns to integers: "
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "metadata": {}, 
            "cell_type": "code", 
            "source": "query = \"\"\"\nselect\n    ID,\n    VAL1,\n    VAL2,\n    simple_function(cast(VAL1 as int)) as s_VAL1,\n    simple_function(cast(VAL2 as int)) as s_VAL2\nfrom\n example2\n\"\"\"\nsqlContext.sql(query).toPandas()"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "That looks better!"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "markdown", 
            "source": "<a id=\"sparkdf\"></a>\n## 5. Convert a pandas DataFrame to a Spark DataFrame\nAlthough pandas DataFrames display data in a friendlier format, Spark DataFrames can be faster and more scalable.\n\nYou'll get a new data set, create a pandas DataFrame for it, and then convert the pandas DataFrame to a Spark DataFrame."
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "<a id=\"ufo\"></a>\n### 5.1 Get a new data set\nGet a data set about UFO sightings:"
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "metadata": {}, 
            "cell_type": "code", 
            "source": "!rm SIGHTINGS.csv -f\n!wget https://ibm.box.com/shared/static/krxz5fo4ojequwb45rcb9lt8fxfcm1vm.csv -O SIGHTINGS.csv"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "<a id=\"ufopandas\"></a>\n### 5.2 Create a pandas DataFrame\nCreate a pandas DataFrame of the data set with the `read_csv` method:"
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "metadata": {}, 
            "cell_type": "code", 
            "source": "# Transform Date Format Lambda from Source CSV\ndateHandle = lambda x: pd.datetime.strptime(x, '%m/%d/%y')\n\npandas_df = pd.read_csv(\"./SIGHTINGS.csv\", parse_dates=['Reports'], date_parser=dateHandle, encoding=\"utf-8-sig\")\n\n# Handle Dates as String to Simplify Spark Example\npandas_df['Reports'] = pandas_df['Reports'].astype(str)\npandas_df.head()"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "<a id=\"sparkufo\"></a>\n### 5.3 Convert to a Spark DataFrame\nConvert the pandas DataFrame to a Spark DataFrame with the `createDataFrame` method. Remember using the `createDataFrame` method to convert an RDD to a Spark DataFrame?"
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "metadata": {}, 
            "cell_type": "code", 
            "source": "spark_df = sqlContext.createDataFrame(pandas_df)"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "Print the first two rows:"
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "metadata": {}, 
            "cell_type": "code", 
            "source": "for row in spark_df.take(2):\n    print row\n"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "Register the Spark DataFrame as a table:"
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "metadata": {}, 
            "cell_type": "code", 
            "source": "spark_df.registerTempTable(\"ufo_sightings\")"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "<a id=\"runufo\"></a>\n### 5.4 Run an SQL statement\nNow run an SQL statement to print the first 10 rows of the table:"
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "metadata": {}, 
            "cell_type": "code", 
            "source": "print sqlContext.sql(\"select * from ufo_sightings limit 10\").collect()"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "<a id=\"viz\"></a>\n## 6. Visualize data\nIt's easy to create charts from pandas DataFrames. You'll use the matplotlib library to create graphs and the NumPy package for computing.\n\nImport the libraries and specify to show graphs inline:"
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "metadata": {}, 
            "cell_type": "code", 
            "source": "%matplotlib inline \nimport matplotlib.pyplot as plt, numpy as np"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "Convert the Spark DataFrame with UFO data to a pandas DataFrame: "
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "metadata": {}, 
            "cell_type": "code", 
            "source": "ufos_df = spark_df.toPandas()"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "<a id=\"vizchart\"></a>\n### 6.1 Create a chart\n\nTo create a chart, you call the `plot()` method and specify the type of chart, the columns for the X and Y axes, and, optionally, the size of the chart. \n\nFor more information about plotting pandas DataFrames, see [Visualization](http://pandas.pydata.org/pandas-docs/stable/visualization.html).\n\nCreate a bar chart 12\" wide by 5\" high that shows the number of reports by date:"
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "metadata": {}, 
            "cell_type": "code", 
            "source": "ufos_df.plot(kind='bar', x='Reports', y='Count', figsize=(12, 5))"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "This chart doesn't look good because there are too many observations. Check how many observations there are by querying the `ufo_sightings` table:"
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "metadata": {}, 
            "cell_type": "code", 
            "source": "print sqlContext.sql(\"select count(*) from ufo_sightings\").collect()"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "<a id=\"vizagg\"></a>\n### 6.2 Aggregate the data\n\nTo reduce the number of data points on the chart, you can aggregate the data by year. Here are a few of the ways that you can do that:\n\n - Run an SQL query the Reports column to output the year and then run a group by operation on the year.\n - Create a simple Python function to aggregate by year and then run the function in an SQL query.\n - Run the `map()` method on the Spark Dataframe to append a new column that contains the aggregated count by year. This is the method you'll use.\n\nRemember that the dates in the Reports column look like this: 2016-01-31. Therefore, to create the year column, you just need the first 4 characters from the Reports column.\n\nAdd a year column to the DataFrame:"
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "metadata": {}, 
            "cell_type": "code", 
            "source": "#ufos_df = spark_df.map(lambda x: Row(**dict(x.asDict(), year=int(x.Reports[0:4])))).toDF()\n# new version\nspark_df.head()\nufos_df = spark_df.rdd.map(lambda x: Row(**dict(x.asDict(), year=int(x.Reports[0:4])))).toDF()"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "Check to verify that you get the expected results:"
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "metadata": {}, 
            "cell_type": "code", 
            "source": "print ufos_df.take(5)"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "Register the DataFrame as a table:"
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "metadata": {}, 
            "cell_type": "code", 
            "source": "ufos_df.registerTempTable(\"ufo_withyear\")"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "<a id=\"vizchart2\"></a>\n### 6.3 Create a better chart\n\nNow run an SQL query to group by year, order by year, and filter to the last 66 years. Then create a pandas DataFrame for the results and create a chart of the number of reports by year."
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "metadata": {}, 
            "cell_type": "code", 
            "source": "query = \"\"\"\nselect \n    sum(Count) as count, \n    year \nfrom ufo_withyear\nwhere year > 1950\ngroup by year\norder by year\n\"\"\"\npandas_ufos_withyears = sqlContext.sql(query).toPandas()\npandas_ufos_withyears.plot(kind='bar', x='year', y='count', figsize=(12, 5))"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "Now you have a chart that you can read. Look back at the original chart and notice that it wasn't ordered ascending by year."
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "markdown", 
            "source": "<a id=\"nextsteps\"></a>\n## 7. Summary and next steps\nYou've learned how to create DataFrames, convert between DataFrame types, and convert from RDDs. You know how to run SQL queries and create SQL functions. And you can visualize the data in charts. \n\nDig deeper:\n - [Apache Spark documentation](http://spark.apache.org/documentation.html)\n - [PySpark documentation](http://spark.apache.org/docs/latest/api/python/pyspark.html)\n - [pandas](http://pandas.pydata.org/pandas-docs/stable/index.html)\n - [matplotlib](http://matplotlib.org/)\n - [NumPy](http://www.numpy.org/)"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "### Authors\n"
        }
    ], 
    "nbformat": 4
}