{
    "nbformat": 4, 
    "cells": [
        {
            "source": "# Feature engineering for semantic analysis\nNatalie Ho | April 2017\n\nThis notebook explores a variety of approaches to common natural language processing (NLP) problems. These techniques will be explained and applied in context of feature engineering a Quora dataset.\n\nI hope that by the end of this notebook, you'll gain familiarity with standard practices as well as recent methods used for NLP tasks. The ever-evolving field has a range of applications from [information retrieval](https://cloud.google.com/natural-language/) to [AI](https://www.ibm.com/developerworks/library/os-ind-watson/), and is well worth a [deeper](https://www.ibm.com/watson/developercloud/doc/natural-language-understanding/index.html) [dive](https://www.ted.com/talks/deb_roy_the_birth_of_a_word).\n\n\n## Table of Contents\n\n1. [Introduction](#bullet-1)<br/>\n    1.1 [Data preview & pre-processing](#bullet-2)<br/>\n    1.2 [What is feature engineering?](#bullet-3)<br/>\n    1.3 [What is NLP?](#bullet-4)<br/>\n<br/>    \n2. [Syntax](#bullet-5)<br/>\n    2.1 [Basic string cleaning](#bullet-6)<br/>\n    2.2 [Simplify question pairs](#bullet-7)<br/>\n    2.3 [Measuring similarity](#bullet-8)<br/>\n<br/>   \n3. [Semantics](#bullet-9)<br/>\n    3.1 [Single word analysis](#bullet-10)<br/>\n    3.2 [Sentence analysis](#bullet-11)<br/>\n    3.3 [Weighted analysis](#bullet-12)<br/>\n    3.4 [Feature creation](#bullet-13)\n    \n    \n\n## 1.0 Introduction<a class=\"anchor\" id=\"bullet-1\"></a>\n\nQuora is a knowledge sharing platform that functions simply on questions and answers. Their mission, plainly stated: \"We want the Quora answer to be the definitive answer for everybody forever.\" In order to ensure the quality of these answers, Quora must protect the integrity of the questions. They accomplish this by adhering to a principle that each logically distinct question should reside on its own page. Unfortunately, the English language is a fickle thing, and intention can vary significantly with subtle shifts in syntactic structure.\n\nOur goal is to create features for syntactically similar, but semantically distinct pairs of strings. We'll be working with Quora's first [public dataset](https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs).\n\n### Load data\n\n###  <span style=\"color: red\"> _User Input_</span>\n\n   - Insert questions.csv to code below as Pandas Data Frame", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "source": "# The code was removed by DSX for sharing.", 
            "metadata": {}, 
            "outputs": []
        }, 
        {
            "source": "### 1.1 Cache Bucket Details for Output Later\nLet's capture our bucket details from our data load so we can save back to that location in the future.\n\n###  <span style=\"color: red\"> _User Input_</span> \n\n- Copy and Paste Bucket Name (Portion after Bucket= in body definition above into bucket variable replacing $BUCKET_NAME)", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "source": "# Define Bucket to Save Project Data \nbucket = '$BUCKET_NAME'\n# Name of Output File for Features\noutfile = 'quora_features.csv'", 
            "metadata": {}, 
            "outputs": []
        }, 
        {
            "source": "### 1.2 Data preview  & pre-processing<a class=\"anchor\" id=\"bullet-2\"></a>\nThe Quora dataset is simple, containing columns for question strings, unique IDs, and a binary variable indicating whether the pair is logically distinct. ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "source": "from IPython.display import display\npd.set_option('display.max_colwidth', -1)\n\n# checking for missing values\ndf.isnull().any()\n\n# drop rows with missing values\ndf=df.dropna()\n\nprint df.shape\ndisplay(df[14:19])", 
            "metadata": {}, 
            "outputs": []
        }, 
        {
            "source": "### 1.3 What is feature engineering?<a class=\"anchor\" id=\"bullet-3\"></a>\n\n**Feature engineering** is the practice of generating data attributes that are useful for prediction. Although the task is loosely defined and depends heavily on the domain in question, it is a key process for optimizing model building. The goal is to find information which best describes the target to be predicted. \n\nIn our case, the target is logical distinction - will one answer suffice for each pair of questions? This target is described by the binary is_duplicate label in the dataset. We will need to process the Quora data to create features that capture the structure and semantics of each question. This will be accomplished by using natural language processing (NLP) methods on the strings. \n\n### 1.4 What is natural language processing?<a class=\"anchor\" id=\"bullet-4\"></a>\nNLP is the field concerned with computational handling of natural language. Grammar is full of seemingly arbitrary exceptions, vocabulary is constantly transforming, and meaning hinges precariously on culture and context. It is no small feat for a machine to find patterns in this dynamic mess (which is somehow easily grasped by the human brain).\n\nWe will start with the simpler task of describing syntax. Skip to [section 3](link) for semantic processing techniques.\n\n## 2.0 Syntax<a class=\"anchor\" id=\"bullet-5\"></a>\n\nA **corpus** is the body of text that we are working with - in this case, the dataset of Quora questions. Our first task is to break the strings down into more manageable units. We will be using the Natural Language Processing Toolkit (NLTK) library to apply the following methods.\n\n\n### 2.1 Basic string cleaning<a class=\"anchor\" id=\"bullet-6\"></a>\n\n**Tokenization**<br/>\nConverting each string into a series of useful units (usually words). We can use NLTK's word_tokenize function to convert a question string into a list of word tokens.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "source": "import nltk\nfrom nltk.tokenize import word_tokenize\n\nnltk.download('punkt')\n\nteststring = df['question1'][12]\ntokens = word_tokenize(df['question1'][12])\n\nprint teststring\nprint tokens", 
            "metadata": {}, 
            "outputs": []
        }, 
        {
            "source": "**Stopwords**<br/>\nCommon words to the corpus that do not significantly alter meaning. The NLTK library includes a set of English language stopwords (e.g. I, you, this, that), which we'll remove from the list of word tokens.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "source": "from nltk.corpus import stopwords\nstop_words = stopwords.words('english')\nstop_words += ['?'] # adding ? character to stop words, since we are working with a corpus of questions\n\nfiltered_tokens = [t for t in tokens if not t in stop_words]\nprint filtered_tokens", 
            "metadata": {}, 
            "outputs": []
        }, 
        {
            "source": "**Stemming**<br/>\nRemoves prefixes and suffixes to extract the **stem** of a word, which may be derived from a **root**. For example, the word \"destabilized\", has the stem \"destablize\", but the root \"stabil-\". The Porter stemming algorithm is often used in practice to handle this task.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "source": "!pip install stemming\nfrom stemming.porter2 import stem\n\nstem_tokens = [stem(t) for t in filtered_tokens]\nprint stem_tokens", 
            "metadata": {}, 
            "outputs": []
        }, 
        {
            "source": "### 2.4 Simplify question pairs <a class=\"anchor\" id=\"bullet-7\"></a>\nWe will combine the string cleaning methods into a function, and apply that across both question columns in the dataset. To prepare for basic comparison, the function will also convert the words to lowercase and sort them alphabetically.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "source": "import string\n\ndef simplify(s):\n    s = str(s).lower().decode('utf-8')\n    tokens = word_tokenize(s)\n    stop_words = stopwords.words('english')\n    stop_words += string.punctuation\n    filtered_tokens = [t for t in tokens if not t in stop_words]\n    stem_tokens = [stem(t) for t in filtered_tokens]\n    sort_tokens = sorted(stem_tokens)\n    if sort_tokens is not []:\n        tokenstr = \" \".join(sort_tokens)\n    else:\n        tokenstr = \"\"\n    return tokenstr.encode('utf-8')\n\ndf['q1_tokens'] = df['question1'].map(simplify)\ndf['q2_tokens'] = df['question2'].map(simplify)\n\nsimplifydf=df[['question1','q1_tokens','question2','q2_tokens','is_duplicate']]\ndisplay(simplifydf[12:13])", 
            "metadata": {}, 
            "outputs": []
        }, 
        {
            "source": "### 2.5 Measuring similarity<a class=\"anchor\" id=\"bullet-8\"></a>\n\nThe simplest way to compare the difference between two strings is by **edit distance**. \n\n**Levenshtein distance**: calculates edit distance by counting the number of operations (add, replace, or delete) that are required to transform one string into another.\n\n**Token sort ratio**: A method from the [FuzzyWuzzy library](http://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/) that uses Levenshtein distance to get the proportion of common tokens between two strings. The score is normalized from 0-100 for easier interpretation.\n\nWe'll create our first two features with these methods.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "source": "!pip install python-Levenshtein\nfrom Levenshtein import distance\n!pip install fuzzywuzzy\nfrom fuzzywuzzy import fuzz\n\ndf['edit_distance'] = df.apply(lambda x: distance(x['q1_tokens'], x['q2_tokens']), axis=1)\ndf['in_common'] = df.apply(lambda x: fuzz.token_sort_ratio(x['q1_tokens'], x['q2_tokens']), axis=1)\n\nsyntaxdf=df[['question1','q1_tokens','question2','q2_tokens','edit_distance','in_common','is_duplicate']]\ndisplay(syntaxdf[508:510]) # example", 
            "metadata": {}, 
            "outputs": []
        }, 
        {
            "source": "Clearly the edit distance or proportion of common tokens is not sufficient to predict duplicate intention. For example, question pair 508 is duplicate, but has a larger edit distance and smaller proportion of common tokens than pair 509.\n\nLet's try to improve on our features by working with semantic methods.\n\n\n## 3.0 Semantics<a class=\"anchor\" id=\"bullet-9\"></a>\n\nTo a machine, words look like characters stored next to one another. Syntax methods allow us to compare words by manipulating them mathematically - counting the number of characters, measuring the amount of work needed to turn one set of characters into another. \n\nSemantic analysis strives to represent how each sequence of characters is related to any other sequence of characters. These relationships can be derived from large bodies of language as a separate machine learning task. A **document** is the group of words in question. In our case, each question from the Quora corpus is one document.\n\nTo start, we'll create lists of word tokens (filtered for stopwords, but not stemmed), to support the methods we'll use in this section.", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "source": "def word_set(s,t,q):\n    s = str(s).lower().decode('utf-8')\n    t = str(t).lower().decode('utf-8')\n    \n    s_tokens, t_tokens = word_tokenize(s), word_tokenize(t)\n    \n    stop_words = stopwords.words('english')\n    stop_words += string.punctuation\n    \n    s_tokens = [x for x in s_tokens if not x in stop_words]\n    t_tokens = [x for x in t_tokens if not x in stop_words]\n    \n    s_temp = set(s_tokens)\n    t_temp = set(t_tokens)\n    \n    s_distinct = [x for x in s_tokens if x not in t_temp]\n    t_distinct = [x for x in t_tokens if x not in s_temp]\n\n    if q == \"q1_words\":\n        return s_tokens\n    elif q == \"q2_words\":\n        return t_tokens\n    elif q == \"q1_distinct\":\n        return s_distinct\n    elif q == \"q2_distinct\":\n        return t_distinct\n\ndf['q1_words'] = df.apply(lambda x: word_set(x['question1'], x['question2'],\"q1_words\"), axis=1)\ndf['q2_words'] = df.apply(lambda x: word_set(x['question1'], x['question2'],\"q2_words\"), axis=1)\n\nwordsdf=df[['question1','q1_words','question2','q2_words','is_duplicate']]\ndisplay(wordsdf[508:510])", 
            "metadata": {}, 
            "outputs": []
        }, 
        {
            "source": "### 3.1 Single word analysis<a class=\"anchor\" id=\"bullet-10\"></a>\n\n**Word embeddings**<br/>\nThis method represents individual words as vectors, and semantic relationships as the distance between vectors. The more related words are, the closer they should exist in vector space. Word embeddings come from the field of [distributional semantics](https://en.wikipedia.org/wiki/Distributional_semantics), which suggests that words are semantically related if they are frequently used in similar contexts (i.e. they are often surrounded by the same words).\n\nFor example, 'Canada' and 'Toronto' should exist closer together in the vector space than 'Canada' and 'Camara' (which would be closer in edit distance).\n\n**Word2Vec**<br/>\nThe mapping of words to vectors is in itself the result of a machine learning algorithm. Developed by Google in 2013, the Word2Vec algorithm is a neural network that takes a large corpus as training data, and produces vector co-ordinates for each word by the word embedding concept. \n\nWe will be using an pre-trained model from Google that was created from over 100 billion words from Google News. The model needs to be [downloaded](https://code.google.com/archive/p/word2vec/) and handled using the [gensim library](https://radimrehurek.com/gensim/) for word vectors. The model is a dictionary that contains every word and its corresponding vector representation, which look like 300 dimensional co-ordinates stored in an array.\n\n**Comparing word vectors**<br/>\nTo compare word vectors, we can use cosine similarity. As the name suggests, this metric measures similarity by taking the cosine of the angle between vectors. The cosine function scales the similarity between 0 and 1, representing words from least to most semantically related.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#### Build Word2Vec Downloader Script\n\nWord2Vec Model needs to be fetched.  It's a rather large file so download may take some time.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "source": "!wget http://bit.ly/2iU22lc\n!mv 2iU22lc GoogleNews-vectors-negative300.bin.gz", 
            "metadata": {}, 
            "outputs": []
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "source": "!pip install gensim\n# download word2vec google model\n#!wget --save-cookies cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=0B7XkCwpI5KDYNlNUTTlSS21pQmM' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/Code: \\1\\n/p'\n#!wget --load-cookies cookies.txt /'https://docs.google.com/uc?export=download&confirm=h10D&id=0B7XkCwpI5KDYNlNUTTlSS21pQmM' -O GoogleNews-vectors-negative300.bin.gz\n\nimport gensim\n\nmodel = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)\n\n# using gensim built-in similarity function for examples\nprint \"Cosine of angle between Canada, Toronto:\" + \"\\n\",\nprint model.similarity('Canada','Toronto')\nprint \"\\n\" + \"Cosine of angle between Canada, Camara:\" + \"\\n\",\nprint model.similarity('Canada','Camara')", 
            "metadata": {}, 
            "outputs": []
        }, 
        {
            "source": "### 3.2 Sentence analysis<a class=\"anchor\" id=\"bullet-11\"></a>\n\nSince the model works like a dictionary, it can only give us vector representations for single words. There are two ways to get a vector representation of a sentence: \n\n1. Train a model on ordered words (e.g. sentences or phrases). Since word order is included during training, the resulting vectors will preserve the relationships between words. I won't be training a new model in this notebook, as it is computationally heavy, but here are some [resources](https://rare-technologies.com/doc2vec-tutorial) for the curious.\n<br/>\n<br/>\n2. Convert a sentence to a set of words, and get the corresponding set of vectors. Averaging the vector set (summing and dividing by total vector length) will give us a single vector that represents that particular set of words. This method can only give a 'bag of words' representation - i.e. word order is not captured.\n\n*Comment: I think that getting new embeddings specific to a corpus is the best-performing method in practice. For the purpose of illustrating NLP problem-solving, I will do my best with bag-of-words methods.*\n\nThe following function implements the second method to get the average embedded vector from a set of words.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "source": "import numpy as np\nfrom __future__ import division\n\ndef vectorize(words):\n    V = np.zeros(300)\n    \n    for w in words:\n        try: \n            V = np.add(V,model[w]) \n        except:\n            continue\n    else:\n        avg_vector = V / np.sqrt((V ** 2).sum())\n        return avg_vector", 
            "metadata": {}, 
            "outputs": []
        }, 
        {
            "source": "Let's see how the average vectors compare for question pair 508:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "source": "from sklearn.metrics.pairwise import cosine_similarity\n\nsent1_q508 = wordsdf['q1_words'][508]\nsent2_q508 = wordsdf['q2_words'][508]\n\nvec1_q508 = vectorize(sent1_q508).reshape(1,-1)\nvec2_q508 = vectorize(sent2_q508).reshape(1,-1)\n\ndisplay(wordsdf[508:509])\n\nprint \"\\n\" + \"Cosine similarity of [best, way, learn, algebra] and [learn, algebra, 1, fast]:\" + \"\\n\",\nprint cosine_similarity(vec1_q508, vec2_q508)[0][0]", 
            "metadata": {}, 
            "outputs": []
        }, 
        {
            "source": "How do the averaged vectors represent the cosine similarities of its components? \n\nIntuitively, if our question pair differs by a closely related word (best vs. ideal) we would get a larger cosine similarity. And if our question pair differs by a very distinct word (algebra vs. juggling), the cosine similarity is smaller.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "source": "# bag of words, so same set of words in a different order does not matter\nprint \"\\n\" + \"Distance between [best, way, learn, algebra] and [learn, algebra, best, way]:\" + \"\\n\",\nprint model.n_similarity(['best','way','learn','algebra'],['learn','algebra','best','way'])\n\n# difference is a semantically similar word\nprint \"\\n\" + \"Distance between [best, way, learn, algebra] and [ideal, way, learn, algebra]:\" + \"\\n\",\nprint model.n_similarity(['best','way','learn','algebra'],['ideal','way','learn','algebra'])\n\n# difference is not semantically similar\nprint \"\\n\" + \"Distance between [best, way, learn, algebra] and [best, way, learn, juggling]:\" + \"\\n\",\nprint model.n_similarity(['best','way','learn','algebra'],['best','way','learn','juggling'])", 
            "metadata": {}, 
            "outputs": []
        }, 
        {
            "source": "**Word mover's distance** <br/>\nAn implementation of Earth mover's distance for natural language processing problems by Kusner et al. <a href=\"#footnote-1\"><sup>[1]</sup></a>\n\nWM distance is an approach that combines the ideas of edit distance with vector representation. It measures the work required to transform one set of vectors into another. Instead of counting edit operations, we use distance between word vectors - how far one vector would have to move to occupy the same spot as the second.\n\nHow Word Mover's Distance is calculated:\n</a><br/><img src=\"https://raw.githubusercontent.com/nllho/quora-nlp/master/images/wmd.PNG\" width=\"400\" height=\"400\"/>\n1. All the words in each set are paired off with each other\n2. Calculate the distance between each pair (instead of cosine similarity, Euclidean distance is used here)\n3. Sum the distances between pairs with minimum distances\n\nIf the two sets do not have the same number of words, the problem becomes an optimization of another measurement called **flow**.\n</a><br/><img src=\"https://raw.githubusercontent.com/nllho/quora-nlp/master/images/flow.PNG\" width=\"320\" height=\"320\"/>\n\n1. The flow is equal to 1/(number of words in the set), so words from the smaller set have a larger flow<br/>\n(words on the bottom have a flow of 0.33, while words on the top have a flow of 0.25)\n2. Extra flow gets attributed to the next most similar words<br/>\n(see the arrows drawn from the bottom words to more than one word in the top row)\n3. The optimization problem identifies the pairs with minimum distances by solving for minimum flow.\n\nWe can use the WM distance method directly from gensim.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "source": "!pip install pyemd\nfrom pyemd import emd\n\nprint \"\\n\" + \"WM distance between [best, way, learn, algebra] and [learn, algebra, 1, fast]:\" + \"\\n\",\nprint model.wmdistance(sent1_q508, sent2_q508)", 
            "metadata": {}, 
            "outputs": []
        }, 
        {
            "source": "### 3.3 Weighted analysis<a class=\"anchor\" id=\"bullet-12\"></a>\n\nIn the example below, we can see that the words are the same except for the name of the country in question (Canada vs. Japan). However, the country name makes all the semantic difference, which we fail to capture using only cosine similarity or WM distance.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "source": "display(wordsdf[14:15])\n\nsent1_q14 = wordsdf['q1_words'][14]\nsent2_q14 = wordsdf['q2_words'][14]\n\nprint \"\\n\" + \"Cosine angle:\" + \"\\n\",\nprint model.n_similarity(sent1_q14, sent2_q14)\n\nprint \"\\n\" + \"WM distance:\" + \"\\n\",\nprint model.wmdistance(sent1_q14, sent2_q14)", 
            "metadata": {}, 
            "outputs": []
        }, 
        {
            "source": "**Weighing uncommon words**<br/>\nLet's assume that 'rare' words are more likely to be semantically significant. We can represent this at the word vector level by multiplying those words by a numerical weight.  \n\n**Term frequency-inverse document frequency** (tf-idf) is a method that assigns weights to word vectors depending on how common they are to a document. The frequency of a word is measured in two ways:\n\n* How many documents contain the word (N)\n* How many times a word appears in one document (f)\n\nThe weight is calculated from the frequency as log(N/f), so the less frequently a word appears in some documents, the higher its weight.\n\nThis method can be implemented via sci-kit learn's built in [Tf-idf Vectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html), which generates weights given a corpus. To save memory and computing time, I decided to simplify the premise of tf-idf for use on pairs of similar questions.\n\n(1) Assume that distinct words  are the most important in telling the difference between question pairs.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "source": "# get list of distinct words for each question\ndf['q1_distinct'] = df.apply(lambda x: word_set(x['question1'], x['question2'],\"q1_distinct\"), axis=1)\ndf['q2_distinct'] = df.apply(lambda x: word_set(x['question1'], x['question2'],\"q2_distinct\"), axis=1)\n\ndistinctdf=df[['question1','q1_words','q1_distinct','question2','q2_words','q2_distinct','is_duplicate']]\ndisplay(distinctdf[14:15])", 
            "metadata": {}, 
            "outputs": []
        }, 
        {
            "source": "It might be useful to get features for the cosine similarity and WM distance for distinct words.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "source": "distinct1 = distinctdf['q1_distinct'][14]\ndistinct2 = distinctdf['q2_distinct'][14]\n\ndistinct_vec1 = vectorize(distinct1).reshape(1,-1)\ndistinct_vec2 = vectorize(distinct2).reshape(1,-1)\n\nprint \"Cosine similarity between distinct vectors ({0}, {1}):\".format(distinct1[0], distinct2[0]) + \"\\n\",\nprint cosine_similarity(distinct_vec1, distinct_vec2)[0][0]\n\nprint \"\\n\" + \"WM distance between distinct vectors ({0}, {1}):\".format(distinct1[0], distinct2[0]) + \"\\n\",\nprint model.wmdistance(distinct1, distinct2)", 
            "metadata": {}, 
            "outputs": []
        }, 
        {
            "source": "(2) Distinct words only appear in one of the two questions, so we can take N = 1. We assumed that distinct words are important, so we assign the distinct words a small frequency of 1/(number of words in the question) for a larger weight.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "source": "# modify vectorize function to add weights\ndef get_weight(words):\n    n = len(words)\n    weight = 1\n    \n    if n != 0:\n        weight = np.log(1/(1/n))\n        \n    return weight", 
            "metadata": {}, 
            "outputs": []
        }, 
        {
            "source": "(3) Generate an array containing the weights for every question in the dataset.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "source": "# empty arrays\nq1_weights = np.zeros((df.shape[0],300))\nq2_weights = np.zeros((df.shape[0],300))\n\n# fill arrays with weights for each question\nfor i, q in enumerate(df.q1_words.values):\n    q1_weights[i, :] = get_weight(q)\n    \nfor i, q in enumerate(df.q2_words.values):\n    q2_weights[i, :] = get_weight(q)", 
            "metadata": {}, 
            "outputs": []
        }, 
        {
            "source": "(4) Calculate the average weighted vectors. We can see how weighing distinct words translates to reduced cosine similarity.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "source": "avg_vec1 = vectorize(sent1_q14).reshape(1,-1)\navg_vec2 = vectorize(sent2_q14).reshape(1,-1)\n\nprint \"\\n\" + \"Cosine similarity between averaged question vectors:\" + \"\\n\",\nprint cosine_similarity(avg_vec1, avg_vec2)[0][0]\n\nw_distinct_vec1 = distinct_vec1*q1_weights[14]\nw_distinct_vec2 = distinct_vec2*q2_weights[14]\n\navg_weight_distinct_vec1 = np.add(avg_vec1, -(distinct_vec1), w_distinct_vec1) \navg_weight_distinct_vec2 = np.add(avg_vec2, -(distinct_vec2), w_distinct_vec2)\n\nprint \"\\n\" + \"Cosine similiarity between weighted question vectors:\" + \"\\n\",\nprint cosine_similarity(avg_weight_distinct_vec1, avg_weight_distinct_vec2)[0][0]", 
            "metadata": {}, 
            "outputs": []
        }, 
        {
            "source": "### 3.4 Feature creation<a class=\"anchor\" id=\"bullet-13\"></a> <a href=\"#footnote-1\"><sup>[2]</sup></a>\nWe can apply these methods to our dataset to create the following features:\n\n* Word mover's distance between sentence sets\n* Word mover's distance between distinct word sets\n* Angle between averaged sentence vectors\n* Angle between averaged distinct word vectors\n* Angle between weighted sentence vectors", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "source": "# word mover's distance between sentence sets\ndf['wm_dist_words'] = df.apply(lambda x: model.wmdistance(x['q1_words'], x['q2_words']), axis=1)\n\n# word mover's distance between distinct sets\ndf['wm_dist_distinct'] = df.apply(lambda x: model.wmdistance(x['q1_distinct'], x['q2_distinct']), axis=1)\n\n# angle between averaged sentence vectors\nq1_avg_vectors = np.zeros((df.shape[0], 300))\nq2_avg_vectors = np.zeros((df.shape[0], 300))\n\nfor i, q in enumerate(df.q1_words.values):\n    q1_avg_vectors[i, :] = vectorize(q)\n\nfor i, q in enumerate(df.q2_words.values):\n    q2_avg_vectors[i, :] = vectorize(q)\n    \n    \ndf['cos_angle_words'] = [cosine_similarity(x.reshape(1,-1), y.reshape(1,-1))[0][0]\n                        for (x, y) in zip(np.nan_to_num(q1_avg_vectors),\n                                          np.nan_to_num(q2_avg_vectors))]\n\n# angle between averaged distinct sentence vectors\nq1_dist_vectors = np.zeros((df.shape[0], 300))\nq2_dist_vectors = np.zeros((df.shape[0], 300))\n\nfor i, q in enumerate(df.q1_distinct.values):\n    q1_dist_vectors[i, :] = vectorize(q)\n\nfor i, q in enumerate(df.q2_distinct.values):\n    q2_dist_vectors[i, :] = vectorize(q)\n    \n    \ndf['cos_angle_distinct'] = [cosine_similarity(x.reshape(1,-1), y.reshape(1,-1))[0][0]\n                           for (x, y) in zip(np.nan_to_num(q1_dist_vectors),\n                                             np.nan_to_num(q2_dist_vectors))]\n\n# get array of weighted distinct vectors\nq1_weight_distinct_vec = np.multiply(q1_dist_vectors,q1_weights)\nq2_weight_distinct_vec = np.multiply(q2_dist_vectors,q2_weights)\n\n# get sentence vectors with weights\nq1_avg_weight_vectors = np.add(q1_avg_vectors, -(q1_dist_vectors), + q1_weight_distinct_vec)\nq2_avg_weight_vectors = np.add(q2_avg_vectors, -(q2_dist_vectors), + q2_weight_distinct_vec)\n\ndf['cos_angle_weighted'] = [cosine_similarity(x.reshape(1,-1), y.reshape(1,-1))[0][0]\n                           for (x, y) in zip(np.nan_to_num(q1_avg_weight_vectors),\n                                             np.nan_to_num(q2_avg_weight_vectors))]\n\ndf[14:15]", 
            "metadata": {}, 
            "outputs": []
        }, 
        {
            "source": "You can now export the feature engineered dataset for use with your preferred model!", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "source": "import StringIO as io\n\nfeaturedf = df.drop(['q1_tokens','q2_tokens','q1_words','q2_words','q1_distinct','q2_distinct'], axis=1)\n\n# Create StringIO object to Stream to Object Store\ncsv_buffer = io.StringIO()\nfeaturedf.to_csv(csv_buffer, index=False)\n\nclient_01da3b8d07aa40ca85ec5cee0637167f.put_object(Body=csv_buffer.getvalue(), Bucket=bucket, Key=outfile)", 
            "metadata": {}, 
            "outputs": []
        }, 
        {
            "source": "## Further reading\n\n* Follow [this tutorial](http://nbviewer.jupyter.org/gist/nllho/4496a06e2bec93f06858851b5d822298) to build an XGBoost classifier, and make predictions using our new features\n* Try [Doc2Vec](https://rare-technologies.com/doc2vec-tutorial) to train a model for sentences or phrases\n* Try [Tf-idf Vectorizer](http://www.markhneedham.com/blog/2015/02/15/pythonscikit-learn-calculating-tfidf-on-how-i-met-your-mother-transcripts/) to generate specific weights based on word frequency in a corpus\n\n\n## References\n\n<p id=\"footnote-1\"><sup>[1]</sup> Kusner, M. J. and Sun, Y. and Kolkin, N. I. and Weinberger, K. Q. (2015) [From Word Embeddings to Document Distances](http://proceedings.mlr.press/v37/kusnerb15.pdf)\n\n<p id=\"footnote-1\"><sup>[2]</sup> Thakur, A. (April 2017) [Is that a duplicate Quora Question?](https://www.linkedin.com/pulse/duplicate-quora-question-abhishek-thakur)", 
            "cell_type": "markdown", 
            "metadata": {}
        }
    ], 
    "metadata": {
        "language_info": {
            "version": "2.7.11", 
            "file_extension": ".py", 
            "codemirror_mode": {
                "version": 2, 
                "name": "ipython"
            }, 
            "nbconvert_exporter": "python", 
            "pygments_lexer": "ipython2", 
            "mimetype": "text/x-python", 
            "name": "python"
        }, 
        "kernelspec": {
            "language": "python", 
            "display_name": "Python 2 with Spark 2.1", 
            "name": "python2-spark21"
        }
    }, 
    "nbformat_minor": 1
}