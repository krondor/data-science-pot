{
    "cells": [
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "![](https://github.com/krondor/data-science-pot/blob/master/dsx_logo.png?raw=true)\n\n# Introduction to NLP: Basic Concepts<a class=\"anchor\" id=\"bullet-1\"></a>\n\nThis notebook guides you through the basic concepts to start working with Natural Language Processing, including how to set up your environment, create and analyze data sets, and work with data files.\n\nThis notebook uses NLTK, a python framework for Natural Language Processing. Some knowledge of Python is recommended. \n\nIf you are new to notebooks, here's how the user interface works: [Parts of a notebook](http://datascience.ibm.com/docs/content/analyze-data/parts-of-a-notebook.html)\n\n## About Natural Language Processing\n\nNatural language processing (NLP) is a field of computer science, artificial intelligence and computational linguistics concerned with the interactions between computers and human (natural) languages, and, in particular, concerned with programming computers to fruitfully process large natural language corpora. Challenges in natural language processing frequently involve natural language understanding, natural language generation (frequently from formal, machine-readable logical forms), connecting language and machine perception, dialog systems, or some combination thereof.\n\n<img src='http://web.stanford.edu/class/cs224n/images/treeFrontSentiment.png' width=\"50%\" height=\"50%\"></img>\n\n## NLP Methods\n\n- Automatic Summarization\n- Translations\n- Named Entity Recognition\n- Natural Language generation\n- Optical Character Recognition (OCR)\n- Part of Speech tagging (POS)\n- Parsing\n- Question Answering\n- Sentiment Analysis\n- Speech Recognition\n- Word sense disambiguation\n- Information Retrieval\n- Stemming\n\n## Table of Contents\n\n1. [Introduction](#bullet-1)<br/>\n<br/>\n2. [Prerequisites](#bullet-2)<br/>\n<br/>\n3. [Preprocessing](#bullet-3)<br/>\n    3.1 [Noise Removal](#bullet-4)<br/>\n    3.2 [Normalization](#bullet-5)<br/>\n    3.3 [Standardization](#bullet-6)<br/>\n<br/> \n4. [Parsing](#bullet-7)<br/>\n    4.1 [Tokenization](#bullet-8)<br/>\n    4.2 [POS Tagging](#bullet-9)<br/>\n    4.3 [Word Sense](#bullet-10)<br/>\n<br/>\n5. [Use Case: Quora Feature Engineering](#bullet-11)<br/>\n    5.1 [Introduction](#bullet-12)<br/>\n    5.1 [Data preview & pre-processing](#bullet-13)<br/>\n    5.2 [What is feature engineering?](#bullet-14)<br/>\n<br/>    \n6. [Syntax](#bullet-15)<br/>\n    6.1 [Basic string cleaning](#bullet-16)<br/>\n    6.2 [Simplify question pairs](#bullet-17)<br/>\n    6.3 [Measuring similarity](#bullet-18)<br/>\n<br/>   \n7. [Semantics](#bullet-19)<br/>\n    7.1 [Single word analysis](#bullet-20)<br/>\n    7.2 [Sentence analysis](#bullet-21)<br/>\n    7.3 [Weighted analysis](#bullet-22)<br/>\n    7.4 [Feature creation](#bullet-23)\n"
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "## Prerequisites<a class=\"anchor\" id=\"bullet-2\"></a>"
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "We'll be working with Quora's first [public dataset](https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs) later in the notebook.\n\n### Load data\n\n###  <span style=\"color: red\"> _User Input_</span>\n\n   - Insert questions.csv to code below as Pandas Data Frame"
        }, 
        {
            "cell_type": "code", 
            "metadata": {}, 
            "source": "# The code was removed by DSX for sharing.", 
            "execution_count": null, 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "### Cache Bucket Details for Output Later\nLet's capture our bucket details from our data load so we can save back to that location in the future.\n\n###  <span style=\"color: red\"> _User Input_</span> \n\n- Copy and Paste Bucket Name (Portion after Bucket= in body definition above into bucket variable replacing $BUCKET_NAME)"
        }, 
        {
            "cell_type": "code", 
            "metadata": {}, 
            "source": "# Define Bucket to Save Project Data \nbucket = '$BUCKET_NAME'\n# Name of Output File for Features\noutfile = 'quora_features.csv'", 
            "execution_count": null, 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "### Install Library Dependencies\n\nThis notebook has a few external library dependencies.  We will install and load these here."
        }, 
        {
            "cell_type": "code", 
            "metadata": {}, 
            "source": "# Install Various Libraries Required\n!pip install fuzzywuzzy\n!pip install gensim\n!pip install nltk\n!pip install pyemd\n!pip install python-Levenshtein\n!pip install stemming", 
            "execution_count": null, 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": ""
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "#### Natural Language Toolkit\n\nPython's Natural Language Toolkit (NLTK) is a comprehensive NLP suite of offerings.  It includes detailed Corpora for a variety of languages and uses.  You can find more detailed information about the Corpora here; [NLTK Corpora](http://www.nltk.org/nltk_data/)\n\n*NLTK Requires Additional Configuration*"
        }, 
        {
            "cell_type": "code", 
            "metadata": {}, 
            "source": "# NLTK Import with Corpus Definition\nimport nltk\n\nnltk.download()\n\nprint \"\\nNLTK Can Also Specify Corpus Manually\"\nprint \"Try nltk.download('popular') for yourselves.\"", 
            "execution_count": null, 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "## Preprocessing<a class=\"anchor\" id=\"bullet-3\"></a>\n\nText is messy data, various types of noise are present in it and the data is not readily analyzable without any pre-processing. The entire process of cleaning and standardization of text, making it noise-free and ready for analysis is known as text preprocessing.\n\nIt is predominantly comprised of three steps:\n\n - Noise Removal\n - Normalization\n - Standardization"
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "### Noise Removal<a class=\"anchor\" id=\"bullet-4\"></a>\n\nAny piece of text which is not relevant to the context of the data and the end-output can be specified as the noise.\n\nFor example \u2013 language stopwords (commonly used words of a language \u2013 is, am, the, of, in etc), URLs or links, social media entities (mentions, hashtags), punctuations and industry specific words. This step deals with removal of all types of noisy entities present in the text.\n\nA general approach for noise removal is to prepare a dictionary of noisy entities, and iterate the text object by tokens (or by words), eliminating those tokens which are present in the noise dictionary.\n\nFollowing is the python code for the same purpose.\n\n#### Stopword Filtering"
        }, 
        {
            "cell_type": "code", 
            "metadata": {}, 
            "source": "noise_list = [\"is\", \"a\", \"this\", \"...\"] \ndef _remove_noise(input_text):\n    words = input_text.split() \n    noise_free_words = [word for word in words if word not in noise_list] \n    noise_free_text = \" \".join(noise_free_words) \n    return noise_free_text\n\n_remove_noise(\"this is a sample text\")", 
            "execution_count": null, 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "### Regex Filtering\n\nAnother approach is to use the regular expressions while dealing with special patterns of noise. We have explained regular expressions in detail in one of our previous article. Following python code removes a regex pattern from the input text:"
        }, 
        {
            "cell_type": "code", 
            "metadata": {}, 
            "source": "import re \n\ndef _remove_regex(input_text, regex_pattern):\n    urls = re.finditer(regex_pattern, input_text) \n    for i in urls: \n        input_text = re.sub(i.group().strip(), '', input_text)\n    return input_text\n\nregex_pattern = \"#[\\w]*\"  \n\n_remove_regex(\"remove this #DSXRocks from tweet text\", regex_pattern)", 
            "execution_count": null, 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "### Normalization<a class=\"anchor\" id=\"bullet-5\"></a>\n\nAnother type of textual noise is about the multiple representations exhibited by single word.\n\nFor example \u2013 \u201cplay\u201d, \u201cplayer\u201d, \u201cplayed\u201d, \u201cplays\u201d and \u201cplaying\u201d are the different variations of the word \u2013 \u201cplay\u201d, Though they mean different but contextually all are similar. The step converts all the disparities of a word into their normalized form (also known as lemma). Normalization is a pivotal step for feature engineering with text as it converts the high dimensional features (N different features) to the low dimensional space (1 feature), which is an ideal ask for any ML model.\n\nThe most common lexicon normalization practices are :\n\n    Stemming:  Stemming is a rudimentary rule-based process of stripping the suffixes (\u201cing\u201d, \u201cly\u201d, \u201ces\u201d, \u201cs\u201d etc) from a word.\n    Lemmatization: Lemmatization, on the other hand, is an organized & step by step procedure of obtaining the root form of the word, \n    it makes use of vocabulary (dictionary importance of words) and morphological analysis (word structure and grammar relations).\n\nBelow is the sample code that performs lemmatization and stemming using python\u2019s popular library \u2013 NLTK."
        }, 
        {
            "cell_type": "code", 
            "metadata": {}, 
            "source": "from nltk.stem.wordnet import WordNetLemmatizer \nlem = WordNetLemmatizer()\n\nfrom nltk.stem.porter import PorterStemmer \nstem = PorterStemmer()\n\nword = \"multiplying\" \nlem.lemmatize(word, \"v\") \nstem.stem(word)", 
            "execution_count": null, 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "### Standardization<a class=\"anchor\" id=\"bullet-6\"></a>\n\nText data often contains words or phrases which are not present in any standard lexical dictionaries. These pieces are not recognized by search engines and models.\n\nSome of the examples are \u2013 acronyms, hashtags with attached words, and colloquial slangs. With the help of regular expressions and manually prepared data dictionaries, this type of noise can be fixed, the code below uses a dictionary lookup method to replace social media slangs from a text."
        }, 
        {
            "cell_type": "code", 
            "metadata": {}, 
            "source": "translation_dict = {'rt':'Retweet', 'dm':'direct message', \"awsm\" : \"awesome\", \"luv\" :\"love\"}", 
            "execution_count": null, 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "## Parsing<a class=\"anchor\" id=\"bullet-7\"></a>\n\nSyntactical parsing involves the analysis of words in the sentence for grammar and their arrangement in a manner that shows the relationships among the words. Dependency Grammar and Part of Speech tags are the important attributes of text syntactics.\n\nDependency Trees \u2013 Sentences are composed of some words sewed together. The relationship among the words in a sentence is determined by the basic dependency grammar. Dependency grammar is a class of syntactic text analysis that deals with (labeled) asymmetrical binary relations between two lexical items (words). Every relation can be represented in the form of a triplet (relation, governor, dependent). For example: consider the sentence \u2013 \u201cBills on ports and immigration were submitted by Senator Brownback, Republican of Kansas.\u201d The relationship among the words can be observed in the form of a tree representation as shown:  \n\n<img src='https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/01/11181146/image-2.png' width=\"50%\" height=\"50%\"></img>\n\nThe tree shows that \u201csubmitted\u201d is the root word of this sentence, and is linked by two sub-trees (subject and object subtrees). Each subtree is a itself a dependency tree with relations such as \u2013 (\u201cBills\u201d <-> \u201cports\u201d <by> \u201cproposition\u201d relation), (\u201cports\u201d <-> \u201cimmigration\u201d <by> \u201cconjugation\u201d relation).\n\nThis type of tree, when parsed recursively in top-down manner gives grammar relation triplets as output which can be used as features for many nlp problems like entity wise sentiment analysis, actor & entity identification, and text classification. The python wrapper StanfordCoreNLP (by Stanford NLP Group, only commercial license) and NLTK dependency grammars can be used to generate dependency trees."
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "### Tokenization<a class=\"anchor\" id=\"bullet-8\"></a>\n\nThe process of splitting text into smaller pieces or units.  We want to tokenize text into sentences, and sentences into tokens.  The library provides a tokenization module, nltk.tokenize"
        }, 
        {
            "cell_type": "code", 
            "metadata": {}, 
            "source": "from nltk import sent_tokenize, word_tokenize\nfrom IPython.display import Image\n\nsentences = sent_tokenize('IBM Data Science Experience (DSX) offers a wealth of functionality to any software ' \\\n                          'developer, especially those interested in data science. An important part of that ' \\\n                          'functionality is the ability to use Notebooks, which are a convenient and intuitive ' \\\n                          'way to compartmentalize different segments of a code base. The IBM Watson Data ' \\\n                          'Platform (WDP) Integration team manages system verification defects for various ' \\\n                          'services and utilizes GitHub\u2019s \u201cissues\u201d feature to keep track of each defect\u2019s ' \\\n                          'status, details, and assignments. Currently, there is no way for us to quantify the ' \\\n                          'team\u2019s activity each week. How many defects are being opened, closed, and worked on ' \\\n                          'each week for each service? How severe are those defects?') \n\nsentences", 
            "execution_count": null, 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "metadata": {}, 
            "source": "tokens = word_tokenize(sentences[2])\ntokens", 
            "execution_count": null, 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "### Part of speech tagging<a class=\"anchor\" id=\"bullet-9\"></a>\n\nApart from the grammar relations, every word in a sentence is also associated with a part of speech (pos) tag (nouns, verbs, adjectives, adverbs etc). The pos tags defines the usage and function of a word in the sentence. Here is a list of all possible pos-tags defined by Pennsylvania university. Following code using NLTK performs pos tagging annotation on input text. (It provides several implementations, the default one is perceptron tagger)"
        }, 
        {
            "cell_type": "code", 
            "metadata": {}, 
            "source": "from nltk import pos_tag \n#this is a Classifier, given a token assign a class\n#pos_tag Already defined in the library. We can train our own. \n\ntags = pos_tag(tokens)\n\ntext = \"I am quickly using Data Science Experience at IBM in Manhattan for natural language processing.\"\ntokens = word_tokenize(text)\nprint pos_tag(tokens)\n\n# Let's apply this to our sample text from our website.\ntags", 
            "execution_count": null, 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "Part of Speech tagging is used for many important purposes in NLP:\n\n#### Word Sense Disambiguation\n\nSome language words have multiple meanings according to their usage. For example, in the two sentences below:\n\nI. \u201cPlease book my flight for Delhi\u201d\n\nII. \u201cI am going to read this book in the flight\u201d\n\n\u201cBook\u201d is used with different context, however the part of speech tag for both of the cases are different. In sentence I, the word \u201cbook\u201d is used as v erb, while in II it is used as no un. (Lesk Algorithm is also used for similar purposes)\n\n#### Improving word-based features\n\nA learning model could learn different contexts of a word when used word as the features, however if the part of speech tag is linked with them, the context is preserved, thus making strong features. For example:\n\nSentence -\u201cbook my flight, I will read this book\u201d\n\nTokens \u2013 (\u201cbook\u201d, 2), (\u201cmy\u201d, 1), (\u201cflight\u201d, 1), (\u201cI\u201d, 1), (\u201cwill\u201d, 1), (\u201cread\u201d, 1), (\u201cthis\u201d, 1)\n\nTokens with POS \u2013 (\u201cbook_VB\u201d, 1), (\u201cmy_PRP$\u201d, 1), (\u201cflight_NN\u201d, 1), (\u201cI_PRP\u201d, 1), (\u201cwill_MD\u201d, 1), (\u201cread_VB\u201d, 1), (\u201cthis_DT\u201d, 1), (\u201cbook_NN\u201d, 1)\n\n#### Normalization and Lemmatization\n\nPOS tags are the basis of lemmatization process for converting a word to its base form (lemma).\n\n#### Efficient Stopword Removal \n\nPOS tags are also useful in efficient removal of stopwords.\n\nFor example, there are some tags which always define the low frequency / less important words of a language. For example: (IN \u2013 \u201cwithin\u201d, \u201cupon\u201d, \u201cexcept\u201d), (CD \u2013 \u201cone\u201d,\u201dtwo\u201d, \u201chundred\u201d), (MD \u2013 \u201cmay\u201d, \u201cmust\u201d etc)\n\n "
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "### Word Senses<a class=\"anchor\" id=\"bullet-10\"></a>\n\nIn linguistics, a word sense is one of the meanings of a word.  Until now, we worked with tokens and POS. So, for instance in \"the man sit down on the bench near the river.\", the token [bench] could be bench as a constructed object by humans where people sit, or the natural side where the river meets the land.\n\n- WordNet: A semantic graph for words. NLTK provides a interface to the API \n\n<img src=\"https://encrypted-tbn3.gstatic.com/images?q=tbn:ANd9GcSFZ2l8g_3316qek21ZEIkTS0WIYs8-lfTvXtO3YGWHEGpdDiMG\">\n\nLets see some functions to handle meanings in tokens. Wordnet provides the concept of synsets, as syntactic units for tokens"
        }, 
        {
            "cell_type": "code", 
            "metadata": {}, 
            "source": "from nltk.corpus import wordnet as wn #loading wordnet module\n\nwn.synsets('human')", 
            "execution_count": null, 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "metadata": {}, 
            "source": "wn.synsets('human')[0].definition ", 
            "execution_count": null, 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "metadata": {}, 
            "source": "wn.synsets('human')[1].definition", 
            "execution_count": null, 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "metadata": {}, 
            "source": "human = wn.synsets('Human',pos=wn.NOUN)[0]\nhuman", 
            "execution_count": null, 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "metadata": {}, 
            "source": "human.hypernyms()", 
            "execution_count": null, 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "metadata": {}, 
            "source": "human.hyponyms() ", 
            "execution_count": null, 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "metadata": {}, 
            "source": "bike = wn.synsets('bicycle')[0]\nbike", 
            "execution_count": null, 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "metadata": {}, 
            "source": "girl = wn.synsets('girl')[1]\ngirl", 
            "execution_count": null, 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "metadata": {}, 
            "source": "bike.wup_similarity(human)", 
            "execution_count": null, 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "metadata": {}, 
            "source": "girl.wup_similarity(human)", 
            "execution_count": null, 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "## Quora Feature engineering for semantic analysis<a class=\"anchor\" id=\"bullet-11\"></a>\n\nThese techniques will be explained and applied in context of feature engineering a Quora dataset.\n\nI hope that by the end of this notebook, you'll gain familiarity with standard practices as well as recent methods used for NLP tasks. The ever-evolving field has a range of applications from [information retrieval](https://cloud.google.com/natural-language/) to [AI](https://www.ibm.com/developerworks/library/os-ind-watson/), and is well worth a [deeper](https://www.ibm.com/watson/developercloud/doc/natural-language-understanding/index.html) [dive](https://www.ted.com/talks/deb_roy_the_birth_of_a_word).\n\n\n## 1.0 Introduction<a class=\"anchor\" id=\"bullet-12\"></a>\n\nQuora is a knowledge sharing platform that functions simply on questions and answers. Their mission, plainly stated: \"We want the Quora answer to be the definitive answer for everybody forever.\" In order to ensure the quality of these answers, Quora must protect the integrity of the questions. They accomplish this by adhering to a principle that each logically distinct question should reside on its own page. Unfortunately, the English language is a fickle thing, and intention can vary significantly with subtle shifts in syntactic structure.\n\nOur goal is to create features for syntactically similar, but semantically distinct pairs of strings. We'll be working with Quora's first [public dataset](https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs)."
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "### 1.1 Data preview  & pre-processing<a class=\"anchor\" id=\"bullet-13\"></a>\nThe Quora dataset is simple, containing columns for question strings, unique IDs, and a binary variable indicating whether the pair is logically distinct. "
        }, 
        {
            "cell_type": "code", 
            "metadata": {}, 
            "source": "from IPython.display import display\npd.set_option('display.max_colwidth', -1)\n\n# checking for missing values\ndf.isnull().any()\n\n# drop rows with missing values\ndf=df.dropna()\n\nprint df.shape\ndisplay(df[14:19])", 
            "execution_count": null, 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "### 1.2 What is feature engineering?<a class=\"anchor\" id=\"bullet-14\"></a>\n\n**Feature engineering** is the practice of generating data attributes that are useful for prediction. Although the task is loosely defined and depends heavily on the domain in question, it is a key process for optimizing model building. The goal is to find information which best describes the target to be predicted. \n\nIn our case, the target is logical distinction - will one answer suffice for each pair of questions? This target is described by the binary is_duplicate label in the dataset. We will need to process the Quora data to create features that capture the structure and semantics of each question. This will be accomplished by using natural language processing (NLP) methods on the strings. "
        }, 
        {
            "cell_type": "code", 
            "metadata": {}, 
            "source": "import nltk\nfrom nltk.tokenize import word_tokenize\n\nnltk.download('punkt')\n\nteststring = df['question1'][12]\ntokens = word_tokenize(df['question1'][12])\n\nprint teststring\nprint tokens", 
            "execution_count": null, 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "### 1.3 Basic String Cleaning<a class=\"anchor\" id=\"bullet-15\"></a>"
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "**Stopwords**<br/>\nThe NLTK library includes a set of English language stopwords (e.g. I, you, this, that), which we'll remove from the list of word tokens."
        }, 
        {
            "cell_type": "code", 
            "metadata": {}, 
            "source": "from nltk.corpus import stopwords\nstop_words = stopwords.words('english')\nstop_words += ['?'] # adding ? character to stop words, since we are working with a corpus of questions\n\nfiltered_tokens = [t for t in tokens if not t in stop_words]\nprint filtered_tokens", 
            "execution_count": null, 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "**Stemming**<br/>\nRemoves prefixes and suffixes to extract the **stem** of a word, which may be derived from a **root**. For example, the word \"destabilized\", has the stem \"destablize\", but the root \"stabil-\". The Porter stemming algorithm is often used in practice to handle this task."
        }, 
        {
            "cell_type": "code", 
            "metadata": {}, 
            "source": "from stemming.porter2 import stem\n\nstem_tokens = [stem(t) for t in filtered_tokens]\nprint stem_tokens", 
            "execution_count": null, 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "### 2.4 Simplify question pairs <a class=\"anchor\" id=\"bullet-15\"></a>\nWe will combine the string cleaning methods into a function, and apply that across both question columns in the dataset. To prepare for basic comparison, the function will also convert the words to lowercase and sort them alphabetically."
        }, 
        {
            "cell_type": "code", 
            "metadata": {}, 
            "source": "import string\n\ndef simplify(s):\n    s = str(s).lower().decode('utf-8')\n    tokens = word_tokenize(s)\n    stop_words = stopwords.words('english')\n    stop_words += string.punctuation\n    filtered_tokens = [t for t in tokens if not t in stop_words]\n    stem_tokens = [stem(t) for t in filtered_tokens]\n    sort_tokens = sorted(stem_tokens)\n    if sort_tokens is not []:\n        tokenstr = \" \".join(sort_tokens)\n    else:\n        tokenstr = \"\"\n    return tokenstr.encode('utf-8')\n\ndf['q1_tokens'] = df['question1'].map(simplify)\ndf['q2_tokens'] = df['question2'].map(simplify)\n\nsimplifydf=df[['question1','q1_tokens','question2','q2_tokens','is_duplicate']]\ndisplay(simplifydf[12:13])", 
            "execution_count": null, 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "### 2.5 Measuring similarity<a class=\"anchor\" id=\"bullet-16\"></a>\n\nThe simplest way to compare the difference between two strings is by **edit distance**. \n\n**Levenshtein distance**: calculates edit distance by counting the number of operations (add, replace, or delete) that are required to transform one string into another.\n\n**Token sort ratio**: A method from the [FuzzyWuzzy library](http://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/) that uses Levenshtein distance to get the proportion of common tokens between two strings. The score is normalized from 0-100 for easier interpretation.\n\nWe'll create our first two features with these methods."
        }, 
        {
            "cell_type": "code", 
            "metadata": {}, 
            "source": "from Levenshtein import distance\nfrom fuzzywuzzy import fuzz\n\ndf['edit_distance'] = df.apply(lambda x: distance(x['q1_tokens'], x['q2_tokens']), axis=1)\ndf['in_common'] = df.apply(lambda x: fuzz.token_sort_ratio(x['q1_tokens'], x['q2_tokens']), axis=1)\n\nsyntaxdf=df[['question1','q1_tokens','question2','q2_tokens','edit_distance','in_common','is_duplicate']]\ndisplay(syntaxdf[508:510]) # example", 
            "execution_count": null, 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }, 
            "source": "Clearly the edit distance or proportion of common tokens is not sufficient to predict duplicate intention. For example, question pair 508 is duplicate, but has a larger edit distance and smaller proportion of common tokens than pair 509.\n\nLet's try to improve on our features by working with semantic methods.\n\n\n## 3.0 Semantics<a class=\"anchor\" id=\"bullet-17\"></a>\n\nTo a machine, words look like characters stored next to one another. Syntax methods allow us to compare words by manipulating them mathematically - counting the number of characters, measuring the amount of work needed to turn one set of characters into another. \n\nSemantic analysis strives to represent how each sequence of characters is related to any other sequence of characters. These relationships can be derived from large bodies of language as a separate machine learning task. A **document** is the group of words in question. In our case, each question from the Quora corpus is one document.\n\nTo start, we'll create lists of word tokens (filtered for stopwords, but not stemmed), to support the methods we'll use in this section."
        }, 
        {
            "cell_type": "code", 
            "metadata": {}, 
            "source": "def word_set(s,t,q):\n    s = str(s).lower().decode('utf-8')\n    t = str(t).lower().decode('utf-8')\n    \n    s_tokens, t_tokens = word_tokenize(s), word_tokenize(t)\n    \n    stop_words = stopwords.words('english')\n    stop_words += string.punctuation\n    \n    s_tokens = [x for x in s_tokens if not x in stop_words]\n    t_tokens = [x for x in t_tokens if not x in stop_words]\n    \n    s_temp = set(s_tokens)\n    t_temp = set(t_tokens)\n    \n    s_distinct = [x for x in s_tokens if x not in t_temp]\n    t_distinct = [x for x in t_tokens if x not in s_temp]\n\n    if q == \"q1_words\":\n        return s_tokens\n    elif q == \"q2_words\":\n        return t_tokens\n    elif q == \"q1_distinct\":\n        return s_distinct\n    elif q == \"q2_distinct\":\n        return t_distinct\n\ndf['q1_words'] = df.apply(lambda x: word_set(x['question1'], x['question2'],\"q1_words\"), axis=1)\ndf['q2_words'] = df.apply(lambda x: word_set(x['question1'], x['question2'],\"q2_words\"), axis=1)\n\nwordsdf=df[['question1','q1_words','question2','q2_words','is_duplicate']]\ndisplay(wordsdf[508:510])", 
            "execution_count": null, 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "### 3.1 Single word analysis<a class=\"anchor\" id=\"bullet-18\"></a>\n\n**Word embeddings**<br/>\nThis method represents individual words as vectors, and semantic relationships as the distance between vectors. The more related words are, the closer they should exist in vector space. Word embeddings come from the field of [distributional semantics](https://en.wikipedia.org/wiki/Distributional_semantics), which suggests that words are semantically related if they are frequently used in similar contexts (i.e. they are often surrounded by the same words).\n\nFor example, 'Canada' and 'Toronto' should exist closer together in the vector space than 'Canada' and 'Camara' (which would be closer in edit distance).\n\n**Word2Vec**<br/>\nThe mapping of words to vectors is in itself the result of a machine learning algorithm. Developed by Google in 2013, the Word2Vec algorithm is a neural network that takes a large corpus as training data, and produces vector co-ordinates for each word by the word embedding concept. \n\nWe will be using an pre-trained model from Google that was created from over 100 billion words from Google News. The model needs to be [downloaded](https://code.google.com/archive/p/word2vec/) and handled using the [gensim library](https://radimrehurek.com/gensim/) for word vectors. The model is a dictionary that contains every word and its corresponding vector representation, which look like 300 dimensional co-ordinates stored in an array.\n\n**Comparing word vectors**<br/>\nTo compare word vectors, we can use cosine similarity. As the name suggests, this metric measures similarity by taking the cosine of the angle between vectors. The cosine function scales the similarity between 0 and 1, representing words from least to most semantically related."
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "#### Build Word2Vec Downloader Script\n\nWord2Vec Model needs to be fetched.  It's a rather large file so download may take some time."
        }, 
        {
            "cell_type": "code", 
            "metadata": {}, 
            "source": "!wget http://bit.ly/2iU22lc\n!mv 2iU22lc GoogleNews-vectors-negative300.bin.gz", 
            "execution_count": null, 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "metadata": {}, 
            "source": "# download word2vec google model\nimport gensim\n\nmodel = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)\n\n# using gensim built-in similarity function for examples\nprint \"Cosine of angle between Canada, Toronto:\" + \"\\n\",\nprint model.similarity('Canada','Toronto')\nprint \"\\n\" + \"Cosine of angle between Canada, Camara:\" + \"\\n\",\nprint model.similarity('Canada','Camara')", 
            "execution_count": null, 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "### 3.2 Sentence analysis<a class=\"anchor\" id=\"bullet-19\"></a>\n\nSince the model works like a dictionary, it can only give us vector representations for single words. There are two ways to get a vector representation of a sentence: \n\n1. Train a model on ordered words (e.g. sentences or phrases). Since word order is included during training, the resulting vectors will preserve the relationships between words. I won't be training a new model in this notebook, as it is computationally heavy, but here are some [resources](https://rare-technologies.com/doc2vec-tutorial) for the curious.\n<br/>\n<br/>\n2. Convert a sentence to a set of words, and get the corresponding set of vectors. Averaging the vector set (summing and dividing by total vector length) will give us a single vector that represents that particular set of words. This method can only give a 'bag of words' representation - i.e. word order is not captured.\n\n*Comment: I think that getting new embeddings specific to a corpus is the best-performing method in practice. For the purpose of illustrating NLP problem-solving, I will do my best with bag-of-words methods.*\n\nThe following function implements the second method to get the average embedded vector from a set of words."
        }, 
        {
            "cell_type": "code", 
            "metadata": {}, 
            "source": "import numpy as np\nfrom __future__ import division\n\ndef vectorize(words):\n    V = np.zeros(300)\n    \n    for w in words:\n        try: \n            V = np.add(V,model[w]) \n        except:\n            continue\n    else:\n        avg_vector = V / np.sqrt((V ** 2).sum())\n        return avg_vector", 
            "execution_count": null, 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "Let's see how the average vectors compare for question pair 508:"
        }, 
        {
            "cell_type": "code", 
            "metadata": {}, 
            "source": "from sklearn.metrics.pairwise import cosine_similarity\n\nsent1_q508 = wordsdf['q1_words'][508]\nsent2_q508 = wordsdf['q2_words'][508]\n\nvec1_q508 = vectorize(sent1_q508).reshape(1,-1)\nvec2_q508 = vectorize(sent2_q508).reshape(1,-1)\n\ndisplay(wordsdf[508:509])\n\nprint \"\\n\" + \"Cosine similarity of [best, way, learn, algebra] and [learn, algebra, 1, fast]:\" + \"\\n\",\nprint cosine_similarity(vec1_q508, vec2_q508)[0][0]", 
            "execution_count": null, 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "How do the averaged vectors represent the cosine similarities of its components? \n\nIntuitively, if our question pair differs by a closely related word (best vs. ideal) we would get a larger cosine similarity. And if our question pair differs by a very distinct word (algebra vs. juggling), the cosine similarity is smaller."
        }, 
        {
            "cell_type": "code", 
            "metadata": {}, 
            "source": "# bag of words, so same set of words in a different order does not matter\nprint \"\\n\" + \"Distance between [best, way, learn, algebra] and [learn, algebra, best, way]:\" + \"\\n\",\nprint model.n_similarity(['best','way','learn','algebra'],['learn','algebra','best','way'])\n\n# difference is a semantically similar word\nprint \"\\n\" + \"Distance between [best, way, learn, algebra] and [ideal, way, learn, algebra]:\" + \"\\n\",\nprint model.n_similarity(['best','way','learn','algebra'],['ideal','way','learn','algebra'])\n\n# difference is not semantically similar\nprint \"\\n\" + \"Distance between [best, way, learn, algebra] and [best, way, learn, juggling]:\" + \"\\n\",\nprint model.n_similarity(['best','way','learn','algebra'],['best','way','learn','juggling'])", 
            "execution_count": null, 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "**Word mover's distance** <br/>\nAn implementation of Earth mover's distance for natural language processing problems by Kusner et al. <a href=\"#footnote-1\"><sup>[1]</sup></a>\n\nWM distance is an approach that combines the ideas of edit distance with vector representation. It measures the work required to transform one set of vectors into another. Instead of counting edit operations, we use distance between word vectors - how far one vector would have to move to occupy the same spot as the second.\n\nHow Word Mover's Distance is calculated:\n</a><br/><img src=\"https://github.com/krondor/data-science-pot/blob/master/wmd.png?raw=TRUE\" width=\"400\" height=\"400\"/>\n1. All the words in each set are paired off with each other\n2. Calculate the distance between each pair (instead of cosine similarity, Euclidean distance is used here)\n3. Sum the distances between pairs with minimum distances\n\nIf the two sets do not have the same number of words, the problem becomes an optimization of another measurement called **flow**.\n\n1. The flow is equal to 1/(number of words in the set), so words from the smaller set have a larger flow<br/>\n(words on the bottom have a flow of 0.33, while words on the top have a flow of 0.25)\n2. Extra flow gets attributed to the next most similar words<br/>\n(see the arrows drawn from the bottom words to more than one word in the top row)\n3. The optimization problem identifies the pairs with minimum distances by solving for minimum flow.\n\nWe can use the WM distance method directly from gensim."
        }, 
        {
            "cell_type": "code", 
            "metadata": {}, 
            "source": "from pyemd import emd\n\nprint \"\\n\" + \"WM distance between [best, way, learn, algebra] and [learn, algebra, 1, fast]:\" + \"\\n\",\nprint model.wmdistance(sent1_q508, sent2_q508)", 
            "execution_count": null, 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "### 3.3 Weighted analysis<a class=\"anchor\" id=\"bullet-20\"></a>\n\nIn the example below, we can see that the words are the same except for the name of the country in question (Canada vs. Japan). However, the country name makes all the semantic difference, which we fail to capture using only cosine similarity or WM distance."
        }, 
        {
            "cell_type": "code", 
            "metadata": {}, 
            "source": "display(wordsdf[14:15])\n\nsent1_q14 = wordsdf['q1_words'][14]\nsent2_q14 = wordsdf['q2_words'][14]\n\nprint \"\\n\" + \"Cosine angle:\" + \"\\n\",\nprint model.n_similarity(sent1_q14, sent2_q14)\n\nprint \"\\n\" + \"WM distance:\" + \"\\n\",\nprint model.wmdistance(sent1_q14, sent2_q14)", 
            "execution_count": null, 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "**Weighing uncommon words**<br/>\nLet's assume that 'rare' words are more likely to be semantically significant. We can represent this at the word vector level by multiplying those words by a numerical weight.  \n\n**Term frequency-inverse document frequency** (tf-idf) is a method that assigns weights to word vectors depending on how common they are to a document. The frequency of a word is measured in two ways:\n\n* How many documents contain the word (N)\n* How many times a word appears in one document (f)\n\nThe weight is calculated from the frequency as log(N/f), so the less frequently a word appears in some documents, the higher its weight.\n\nThis method can be implemented via sci-kit learn's built in [Tf-idf Vectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html), which generates weights given a corpus. To save memory and computing time, I decided to simplify the premise of tf-idf for use on pairs of similar questions.\n\n(1) Assume that distinct words  are the most important in telling the difference between question pairs."
        }, 
        {
            "cell_type": "code", 
            "metadata": {}, 
            "source": "# get list of distinct words for each question\ndf['q1_distinct'] = df.apply(lambda x: word_set(x['question1'], x['question2'],\"q1_distinct\"), axis=1)\ndf['q2_distinct'] = df.apply(lambda x: word_set(x['question1'], x['question2'],\"q2_distinct\"), axis=1)\n\ndistinctdf=df[['question1','q1_words','q1_distinct','question2','q2_words','q2_distinct','is_duplicate']]\ndisplay(distinctdf[14:15])", 
            "execution_count": null, 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "It might be useful to get features for the cosine similarity and WM distance for distinct words."
        }, 
        {
            "cell_type": "code", 
            "metadata": {}, 
            "source": "distinct1 = distinctdf['q1_distinct'][14]\ndistinct2 = distinctdf['q2_distinct'][14]\n\ndistinct_vec1 = vectorize(distinct1).reshape(1,-1)\ndistinct_vec2 = vectorize(distinct2).reshape(1,-1)\n\nprint \"Cosine similarity between distinct vectors ({0}, {1}):\".format(distinct1[0], distinct2[0]) + \"\\n\",\nprint cosine_similarity(distinct_vec1, distinct_vec2)[0][0]\n\nprint \"\\n\" + \"WM distance between distinct vectors ({0}, {1}):\".format(distinct1[0], distinct2[0]) + \"\\n\",\nprint model.wmdistance(distinct1, distinct2)", 
            "execution_count": null, 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "(2) Distinct words only appear in one of the two questions, so we can take N = 1. We assumed that distinct words are important, so we assign the distinct words a small frequency of 1/(number of words in the question) for a larger weight."
        }, 
        {
            "cell_type": "code", 
            "metadata": {}, 
            "source": "# modify vectorize function to add weights\ndef get_weight(words):\n    n = len(words)\n    weight = 1\n    \n    if n != 0:\n        weight = np.log(1/(1/n))\n        \n    return weight", 
            "execution_count": null, 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "(3) Generate an array containing the weights for every question in the dataset."
        }, 
        {
            "cell_type": "code", 
            "metadata": {}, 
            "source": "# empty arrays\nq1_weights = np.zeros((df.shape[0],300))\nq2_weights = np.zeros((df.shape[0],300))\n\n# fill arrays with weights for each question\nfor i, q in enumerate(df.q1_words.values):\n    q1_weights[i, :] = get_weight(q)\n    \nfor i, q in enumerate(df.q2_words.values):\n    q2_weights[i, :] = get_weight(q)", 
            "execution_count": null, 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "(4) Calculate the average weighted vectors. We can see how weighing distinct words translates to reduced cosine similarity."
        }, 
        {
            "cell_type": "code", 
            "metadata": {}, 
            "source": "avg_vec1 = vectorize(sent1_q14).reshape(1,-1)\navg_vec2 = vectorize(sent2_q14).reshape(1,-1)\n\nprint \"\\n\" + \"Cosine similarity between averaged question vectors:\" + \"\\n\",\nprint cosine_similarity(avg_vec1, avg_vec2)[0][0]\n\nw_distinct_vec1 = distinct_vec1*q1_weights[14]\nw_distinct_vec2 = distinct_vec2*q2_weights[14]\n\navg_weight_distinct_vec1 = np.add(avg_vec1, -(distinct_vec1), w_distinct_vec1) \navg_weight_distinct_vec2 = np.add(avg_vec2, -(distinct_vec2), w_distinct_vec2)\n\nprint \"\\n\" + \"Cosine similiarity between weighted question vectors:\" + \"\\n\",\nprint cosine_similarity(avg_weight_distinct_vec1, avg_weight_distinct_vec2)[0][0]", 
            "execution_count": null, 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "### 3.4 Feature creation<a class=\"anchor\" id=\"bullet-21\"></a> <a href=\"#footnote-1\"><sup>[2]</sup></a>\nWe can apply these methods to our dataset to create the following features:\n\n* Word mover's distance between sentence sets\n* Word mover's distance between distinct word sets\n* Angle between averaged sentence vectors\n* Angle between averaged distinct word vectors\n* Angle between weighted sentence vectors"
        }, 
        {
            "cell_type": "code", 
            "metadata": {}, 
            "source": "# word mover's distance between sentence sets\ndf['wm_dist_words'] = df.apply(lambda x: model.wmdistance(x['q1_words'], x['q2_words']), axis=1)\n\n# word mover's distance between distinct sets\ndf['wm_dist_distinct'] = df.apply(lambda x: model.wmdistance(x['q1_distinct'], x['q2_distinct']), axis=1)\n\n# angle between averaged sentence vectors\nq1_avg_vectors = np.zeros((df.shape[0], 300))\nq2_avg_vectors = np.zeros((df.shape[0], 300))\n\nfor i, q in enumerate(df.q1_words.values):\n    q1_avg_vectors[i, :] = vectorize(q)\n\nfor i, q in enumerate(df.q2_words.values):\n    q2_avg_vectors[i, :] = vectorize(q)\n    \n    \ndf['cos_angle_words'] = [cosine_similarity(x.reshape(1,-1), y.reshape(1,-1))[0][0]\n                        for (x, y) in zip(np.nan_to_num(q1_avg_vectors),\n                                          np.nan_to_num(q2_avg_vectors))]\n\n# angle between averaged distinct sentence vectors\nq1_dist_vectors = np.zeros((df.shape[0], 300))\nq2_dist_vectors = np.zeros((df.shape[0], 300))\n\nfor i, q in enumerate(df.q1_distinct.values):\n    q1_dist_vectors[i, :] = vectorize(q)\n\nfor i, q in enumerate(df.q2_distinct.values):\n    q2_dist_vectors[i, :] = vectorize(q)\n    \n    \ndf['cos_angle_distinct'] = [cosine_similarity(x.reshape(1,-1), y.reshape(1,-1))[0][0]\n                           for (x, y) in zip(np.nan_to_num(q1_dist_vectors),\n                                             np.nan_to_num(q2_dist_vectors))]\n\n# get array of weighted distinct vectors\nq1_weight_distinct_vec = np.multiply(q1_dist_vectors,q1_weights)\nq2_weight_distinct_vec = np.multiply(q2_dist_vectors,q2_weights)\n\n# get sentence vectors with weights\nq1_avg_weight_vectors = np.add(q1_avg_vectors, -(q1_dist_vectors), + q1_weight_distinct_vec)\nq2_avg_weight_vectors = np.add(q2_avg_vectors, -(q2_dist_vectors), + q2_weight_distinct_vec)\n\ndf['cos_angle_weighted'] = [cosine_similarity(x.reshape(1,-1), y.reshape(1,-1))[0][0]\n                           for (x, y) in zip(np.nan_to_num(q1_avg_weight_vectors),\n                                             np.nan_to_num(q2_avg_weight_vectors))]\n\ndf[14:15]", 
            "execution_count": null, 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "You can now export the feature engineered dataset for use with your preferred model!"
        }, 
        {
            "cell_type": "code", 
            "metadata": {}, 
            "source": "import StringIO as io\n\nfeaturedf = df.drop(['q1_tokens','q2_tokens','q1_words','q2_words','q1_distinct','q2_distinct'], axis=1)\n\n# Create StringIO object to Stream to Object Store\ncsv_buffer = io.StringIO()\nfeaturedf.to_csv(csv_buffer, index=False)\n\nclient_01da3b8d07aa40ca85ec5cee0637167f.put_object(Body=csv_buffer.getvalue(), Bucket=bucket, Key=outfile)", 
            "execution_count": null, 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "Natalie Ho | April 2017\n\nRyan Kather | November 2017\n\n## Further reading\n\n* Follow [this tutorial](http://nbviewer.jupyter.org/gist/nllho/4496a06e2bec93f06858851b5d822298) to build an XGBoost classifier, and make predictions using our new features\n* Try [Doc2Vec](https://rare-technologies.com/doc2vec-tutorial) to train a model for sentences or phrases\n* Try [Tf-idf Vectorizer](http://www.markhneedham.com/blog/2015/02/15/pythonscikit-learn-calculating-tfidf-on-how-i-met-your-mother-transcripts/) to generate specific weights based on word frequency in a corpus\n\n\n## References\n\n<p id=\"footnote-1\"><sup>[1]</sup> Kusner, M. J. and Sun, Y. and Kolkin, N. I. and Weinberger, K. Q. (2015) [From Word Embeddings to Document Distances](http://proceedings.mlr.press/v37/kusnerb15.pdf)\n\n<p id=\"footnote-1\"><sup>[2]</sup> Thakur, A. (April 2017) [Is that a duplicate Quora Question?](https://www.linkedin.com/pulse/duplicate-quora-question-abhishek-thakur)"
        }, 
        {
            "cell_type": "code", 
            "metadata": {}, 
            "source": "", 
            "execution_count": null, 
            "outputs": []
        }
    ], 
    "nbformat": 4, 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 2 with Spark 2.1", 
            "language": "python", 
            "name": "python2-spark21"
        }, 
        "language_info": {
            "version": "2.7.11", 
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "file_extension": ".py", 
            "codemirror_mode": {
                "version": 2, 
                "name": "ipython"
            }, 
            "name": "python", 
            "pygments_lexer": "ipython2"
        }
    }, 
    "nbformat_minor": 1
}